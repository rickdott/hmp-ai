{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Pre-processing and frequency sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Is it possible to train a model on unpreprocessed EEG data and still attain similar performance levels?\n",
    "\n",
    "**Hypothesis**: The model will perform worse, but if still similar then the added value of not having to (manually) preprocess EEG data is very valuable and opens up a multitude of applications.\n",
    "\n",
    "**Result**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Preparing data\n",
    "To use hmp.utils.read_mne_data() and epoch the information, the files should be in .fif format, this replicates automated preprocessing as done in https://github.com/GWeindel/hsmm_mvpy/blob/main/tutorials/sample_data/eeg/0022.ipynb excepting resampling to 100Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 11:28:17.094579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-20 11:28:17.746002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "from pathlib import Path\n",
    "import hsmm_mvpy as hmp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from shared.data import add_stage_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths and file locations\n",
    "data_path = Path(\"/mnt/d/thesis/sat1/\")\n",
    "behavioral_data_path = data_path / \"ExperimentData/ExperimentData\"\n",
    "output_path = Path(\"data/sat1/unpreprocessed\")\n",
    "\n",
    "subj_ids = [\n",
    "    subj_id.name.split(\"-\")[1][:4] for subj_id in (data_path / \"eeg4\").glob(\"*.vhdr\")\n",
    "]\n",
    "subj_files = [\n",
    "    str(output_path / f\"unprocessed_{subj_id}_epo.fif\") for subj_id in subj_ids\n",
    "]\n",
    "behavioral_files = [\n",
    "    str(behavioral_data_path / f\"{subj_id}-cnv-sat3_ET.csv\") for subj_id in subj_ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing preprocessing done in https://github.com/GWeindel/hsmm_mvpy/blob/main/tutorials/sample_data/eeg/0022.ipynb\n",
    "# with only the necessary (non-manual) parts, like adding metadata for processing in HMP package, more info in link above\n",
    "for subject_id in subj_ids:\n",
    "    print(f\"Processing subject: {subject_id}\")\n",
    "    subject_id_short = subject_id.replace(\"0\", \"\")\n",
    "    raw = mne.io.read_raw_brainvision(\n",
    "        data_path / \"eeg4\" / f\"MD3-{subject_id}.vhdr\", preload=False\n",
    "    )\n",
    "    raw.set_channel_types(\n",
    "        {\"EOGh\": \"eog\", \"EOGv\": \"eog\", \"A1\": \"misc\", \"A2\": \"misc\"}\n",
    "    )  # Declare type to avoid confusion with EEG channels\n",
    "    raw.rename_channels({\"FP1\": \"Fp1\", \"FP2\": \"Fp2\"})  # Naming convention\n",
    "    raw.set_montage(\"standard_1020\")  # Standard 10-20 electrode montage\n",
    "    raw.rename_channels({\"Fp1\": \"FP1\", \"Fp2\": \"FP2\"})\n",
    "\n",
    "    behavioral_path = behavioral_data_path / f\"{subject_id}-cnv-sat3_ET.csv\"\n",
    "    behavior = pd.read_csv(behavioral_path, sep=\";\")[\n",
    "        [\n",
    "            \"stim\",\n",
    "            \"resp\",\n",
    "            \"RT\",\n",
    "            \"cue\",\n",
    "            \"movement\",\n",
    "        ]\n",
    "    ]\n",
    "    behavior[\"movement\"] = behavior.apply(\n",
    "        lambda row: \"stim_left\"\n",
    "        if row[\"movement\"] == -1\n",
    "        else (\"stim_right\" if row[\"movement\"] == 1 else np.nan),\n",
    "        axis=1,\n",
    "    )\n",
    "    behavior[\"resp\"] = behavior.apply(\n",
    "        lambda row: \"resp_left\"\n",
    "        if row[\"resp\"] == 1\n",
    "        else (\"resp_right\" if row[\"resp\"] == 2 else np.nan),\n",
    "        axis=1,\n",
    "    )\n",
    "    # Merging together the exeperimental conditions info to have the format condition/stimulus/response\n",
    "    behavior[\"trigger\"] = (\n",
    "        behavior[\"cue\"] + \"/\" + behavior[\"movement\"] + \"/\" + behavior[\"resp\"]\n",
    "    )\n",
    "    # Filtering out < 300 and > 3000 Reaction times\n",
    "    behavior[\"RT\"] = behavior.apply(\n",
    "        lambda row: 0\n",
    "        if row[\"RT\"] < 300\n",
    "        else (0 if row[\"RT\"] > 3000 else float(row[\"RT\"]) / 1000),\n",
    "        axis=1,\n",
    "    )\n",
    "    epochs = mne.io.read_epochs_fieldtrip(\n",
    "        data_path / \"eeg1\" / f\"data{subject_id_short}.mat\", info=raw.info\n",
    "    )\n",
    "    epochs.rename_channels({\"FP1\": \"Fp1\", \"FP2\": \"Fp2\"})  # Naming convention\n",
    "    epochs.set_montage(\"easycap-M1\")\n",
    "    epochs.filter(1, 35)  # Bandwidth filter from van Maanen, Portoles & Borst (2021)\n",
    "    epochs.crop(tmin=-0.250)\n",
    "    epochs.set_eeg_reference(\"average\")\n",
    "    epochs.metadata = behavior\n",
    "    epochs.save(\n",
    "        output_path / f\"unprocessed_{subject_id}_epo.fif\", overwrite=True, verbose=False\n",
    "    )  # Saving EEG mne format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing participant data/sat1/unpreprocessed/unprocessed_0001_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0001_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0002_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0002_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0003_epo.fif's epoched eeg\n",
      "191 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0003_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0004_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0004_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0005_epo.fif's epoched eeg\n",
      "190 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0005_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0006_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0006_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0007_epo.fif's epoched eeg\n",
      "199 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0007_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0008_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0008_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0009_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0009_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0010_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0010_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0011_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0011_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0012_epo.fif's epoched eeg\n",
      "188 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0012_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0013_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0013_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0014_epo.fif's epoched eeg\n",
      "186 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0014_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0015_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0015_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0016_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0016_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0017_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0017_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0018_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0018_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0019_epo.fif's epoched eeg\n",
      "197 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0019_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0020_epo.fif's epoched eeg\n",
      "178 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0020_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0021_epo.fif's epoched eeg\n",
      "199 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0021_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0022_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0022_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0023_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0023_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0024_epo.fif's epoched eeg\n",
      "185 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0024_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0025_epo.fif's epoched eeg\n",
      "199 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0025_epo.fif\n"
     ]
    }
   ],
   "source": [
    "output_path_data = Path(\"data/sat1/data_unprocessed_500hz.nc\")\n",
    "# Run if data_unprocessed.nc does not exist or should be rewritten\n",
    "data = hmp.utils.read_mne_data(\n",
    "    subj_files,\n",
    "    epoched=True,\n",
    "    lower_limit_RT=0.2,\n",
    "    upper_limit_RT=2,\n",
    "    verbose=False,\n",
    "    subj_idx=subj_ids,\n",
    "    rt_col=\"RT\",\n",
    ")\n",
    "data.to_netcdf(output_path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing participant data/sat1/unpreprocessed/unprocessed_0001_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0001_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0002_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0002_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0003_epo.fif's epoched eeg\n",
      "191 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0003_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0004_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0004_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0005_epo.fif's epoched eeg\n",
      "190 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0005_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0006_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0006_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0007_epo.fif's epoched eeg\n",
      "199 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0007_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0008_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0008_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0009_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0009_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0010_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0010_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0011_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0011_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0012_epo.fif's epoched eeg\n",
      "188 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0012_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0013_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0013_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0014_epo.fif's epoched eeg\n",
      "186 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0014_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0015_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0015_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0016_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0016_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0017_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0017_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0018_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0018_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0019_epo.fif's epoched eeg\n",
      "197 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0019_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0020_epo.fif's epoched eeg\n",
      "178 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0020_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0021_epo.fif's epoched eeg\n",
      "199 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0021_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0022_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0022_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0023_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0023_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0024_epo.fif's epoched eeg\n",
      "185 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0024_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0025_epo.fif's epoched eeg\n",
      "199 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0025_epo.fif\n"
     ]
    }
   ],
   "source": [
    "output_path_data = Path(\"data/sat1/data_unprocessed_100hz.nc\")\n",
    "# Run if data_unprocessed.nc does not exist or should be rewritten\n",
    "data = hmp.utils.read_mne_data(\n",
    "    subj_files,\n",
    "    epoched=True,\n",
    "    lower_limit_RT=0.2,\n",
    "    upper_limit_RT=2,\n",
    "    sfreq=100,\n",
    "    verbose=False,\n",
    "    subj_idx=subj_ids,\n",
    "    rt_col=\"RT\",\n",
    ")\n",
    "data.to_netcdf(output_path_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use information from stage_data to split unprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 500Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding stage changes\n",
      "Combining segments\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"data/sat1/stage_data.nc\")\n",
    "merge_dataset = xr.load_dataset(Path(\"data/sat1/data_unprocessed_500hz.nc\"))\n",
    "output_data = add_stage_dimension(data_path, merge_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"data/sat1/split_stage_data_unprocessed_500hz.nc\")\n",
    "output_data.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 100Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding stage changes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining segments\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"data/sat1/stage_data.nc\")\n",
    "merge_dataset = xr.load_dataset(Path(\"data/sat1/data_unprocessed_100hz.nc\"))\n",
    "output_data = add_stage_dimension(data_path, merge_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"data/sat1/split_stage_data_unprocessed_100hz.nc\")\n",
    "output_data.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 12:47:18.316688: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-20 12:47:19.035129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n",
      "env: TF_GPU_ALLOCATOR=cuda_malloc_async\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from shared.data import add_stage_dimension\n",
    "from shared.training import split_data_on_participants, train_and_evaluate, k_fold_cross_validate, get_compile_kwargs\n",
    "from shared.normalization import *\n",
    "from shared.models import SAT1Base, SAT1Topological, SAT1Deep\n",
    "from shared.utilities import print_results\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "%env TF_GPU_ALLOCATOR=cuda_malloc_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = Path(\"logs/exp_preprocessing/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2a: Processed 100Hz (control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/sat1/split_stage_data.nc\")\n",
    "data = xr.load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 17:46:44.898784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 17:46:44.931234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 17:46:44.931331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 17:46:44.933800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 17:46:44.933875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 17:46:44.933930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 17:46:45.414395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 17:46:45.414482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 17:46:45.414491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-17 17:46:45.414548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 17:46:45.414559: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-10-17 17:46:45.414564: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2023-10-17 17:46:45.414675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21598 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: test fold: ['0009' '0017' '0001' '0024' '0012']\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 17:46:48.212527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-10-17 17:46:48.893596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-10-17 17:46:49.215032: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556587cdee30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-17 17:46:49.215067: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-10-17 17:46:49.221195: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-17 17:46:49.328577: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997/997 [==============================] - 17s 13ms/step - loss: 3.6975 - accuracy: 0.7287 - val_loss: 0.5757 - val_accuracy: 0.7936\n",
      "Epoch 2/20\n",
      "997/997 [==============================] - 14s 13ms/step - loss: 2.8072 - accuracy: 0.7963 - val_loss: 0.5160 - val_accuracy: 0.8086\n",
      "Epoch 3/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 2.4092 - accuracy: 0.8225 - val_loss: 0.5079 - val_accuracy: 0.8093\n",
      "Epoch 4/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 2.1977 - accuracy: 0.8389 - val_loss: 0.4915 - val_accuracy: 0.8155\n",
      "Epoch 5/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 1.9734 - accuracy: 0.8527 - val_loss: 0.4765 - val_accuracy: 0.8204\n",
      "Epoch 6/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 1.8409 - accuracy: 0.8664 - val_loss: 0.4750 - val_accuracy: 0.8265\n",
      "Epoch 7/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 1.6335 - accuracy: 0.8769 - val_loss: 0.5327 - val_accuracy: 0.8199\n",
      "Epoch 8/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 1.5267 - accuracy: 0.8880 - val_loss: 0.5130 - val_accuracy: 0.8216\n",
      "Epoch 9/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 1.3483 - accuracy: 0.8974 - val_loss: 0.5155 - val_accuracy: 0.8322\n",
      "254/254 [==============================] - 3s 10ms/step\n",
      "Fold 1: accuracy: 0.8265255905511811\n",
      "Fold 2: test fold: ['0010' '0014' '0002' '0023' '0006']\n",
      "Epoch 1/20\n",
      "995/995 [==============================] - 14s 14ms/step - loss: 3.8727 - accuracy: 0.7290 - val_loss: 0.5923 - val_accuracy: 0.7805\n",
      "Epoch 2/20\n",
      "995/995 [==============================] - 14s 14ms/step - loss: 2.6737 - accuracy: 0.8013 - val_loss: 0.4755 - val_accuracy: 0.8218\n",
      "Epoch 3/20\n",
      "995/995 [==============================] - 14s 13ms/step - loss: 2.4567 - accuracy: 0.8184 - val_loss: 0.4463 - val_accuracy: 0.8323\n",
      "Epoch 4/20\n",
      "995/995 [==============================] - 14s 14ms/step - loss: 2.2811 - accuracy: 0.8282 - val_loss: 0.4512 - val_accuracy: 0.8328\n",
      "Epoch 5/20\n",
      "995/995 [==============================] - 14s 14ms/step - loss: 2.2080 - accuracy: 0.8352 - val_loss: 0.4537 - val_accuracy: 0.8352\n",
      "Epoch 6/20\n",
      "995/995 [==============================] - 14s 14ms/step - loss: 2.0390 - accuracy: 0.8455 - val_loss: 0.4554 - val_accuracy: 0.8342\n",
      "Epoch 7/20\n",
      "995/995 [==============================] - 14s 14ms/step - loss: 1.9194 - accuracy: 0.8571 - val_loss: 0.4463 - val_accuracy: 0.8394\n",
      "Epoch 8/20\n",
      "995/995 [==============================] - 14s 14ms/step - loss: 1.8069 - accuracy: 0.8635 - val_loss: 0.4530 - val_accuracy: 0.8447\n",
      "Epoch 9/20\n",
      "995/995 [==============================] - 14s 14ms/step - loss: 1.6828 - accuracy: 0.8735 - val_loss: 0.4679 - val_accuracy: 0.8418\n",
      "Epoch 10/20\n",
      "995/995 [==============================] - 14s 14ms/step - loss: 1.5931 - accuracy: 0.8778 - val_loss: 0.4430 - val_accuracy: 0.8438\n",
      "Epoch 11/20\n",
      "995/995 [==============================] - 14s 14ms/step - loss: 1.4559 - accuracy: 0.8869 - val_loss: 0.4841 - val_accuracy: 0.8491\n",
      "Epoch 12/20\n",
      "995/995 [==============================] - 14s 14ms/step - loss: 1.3466 - accuracy: 0.8978 - val_loss: 0.5389 - val_accuracy: 0.8369\n",
      "Epoch 13/20\n",
      "995/995 [==============================] - 14s 14ms/step - loss: 1.2917 - accuracy: 0.9004 - val_loss: 0.5121 - val_accuracy: 0.8411\n",
      "256/256 [==============================] - 3s 11ms/step\n",
      "Fold 2: accuracy: 0.84375\n",
      "Fold 3: test fold: ['0003' '0013' '0016' '0004' '0005']\n",
      "Epoch 1/20\n",
      "1002/1002 [==============================] - 14s 14ms/step - loss: 4.1664 - accuracy: 0.6975 - val_loss: 0.5973 - val_accuracy: 0.7676\n",
      "Epoch 2/20\n",
      "1002/1002 [==============================] - 14s 14ms/step - loss: 3.0478 - accuracy: 0.7765 - val_loss: 0.5512 - val_accuracy: 0.7846\n",
      "Epoch 3/20\n",
      "1002/1002 [==============================] - 14s 14ms/step - loss: 2.6682 - accuracy: 0.8023 - val_loss: 0.5000 - val_accuracy: 0.8057\n",
      "Epoch 4/20\n",
      "1002/1002 [==============================] - 14s 14ms/step - loss: 2.4336 - accuracy: 0.8223 - val_loss: 0.4922 - val_accuracy: 0.8085\n",
      "Epoch 5/20\n",
      "1002/1002 [==============================] - 14s 14ms/step - loss: 2.2648 - accuracy: 0.8299 - val_loss: 0.4918 - val_accuracy: 0.8100\n",
      "Epoch 6/20\n",
      "1002/1002 [==============================] - 14s 14ms/step - loss: 2.1633 - accuracy: 0.8381 - val_loss: 0.5074 - val_accuracy: 0.8095\n",
      "Epoch 7/20\n",
      "1002/1002 [==============================] - 14s 14ms/step - loss: 2.0062 - accuracy: 0.8489 - val_loss: 0.5611 - val_accuracy: 0.7994\n",
      "Epoch 8/20\n",
      "1002/1002 [==============================] - 14s 14ms/step - loss: 1.8843 - accuracy: 0.8552 - val_loss: 0.4908 - val_accuracy: 0.8175\n",
      "Epoch 9/20\n",
      "1002/1002 [==============================] - 14s 14ms/step - loss: 1.8041 - accuracy: 0.8641 - val_loss: 0.5002 - val_accuracy: 0.8215\n",
      "Epoch 10/20\n",
      "1002/1002 [==============================] - 14s 14ms/step - loss: 1.6816 - accuracy: 0.8718 - val_loss: 0.5391 - val_accuracy: 0.8090\n",
      "Epoch 11/20\n",
      "1002/1002 [==============================] - 14s 14ms/step - loss: 1.5958 - accuracy: 0.8789 - val_loss: 0.5909 - val_accuracy: 0.8050\n",
      "249/249 [==============================] - 3s 11ms/step\n",
      "Fold 3: accuracy: 0.8175200803212851\n",
      "Fold 4: test fold: ['0021' '0018' '0022' '0019' '0025']\n",
      "Epoch 1/20\n",
      "1011/1011 [==============================] - 14s 14ms/step - loss: 3.9305 - accuracy: 0.7184 - val_loss: 0.5009 - val_accuracy: 0.8169\n",
      "Epoch 2/20\n",
      "1011/1011 [==============================] - 14s 14ms/step - loss: 2.8161 - accuracy: 0.7941 - val_loss: 0.5508 - val_accuracy: 0.7884\n",
      "Epoch 3/20\n",
      "1011/1011 [==============================] - 14s 14ms/step - loss: 2.5596 - accuracy: 0.8089 - val_loss: 0.4586 - val_accuracy: 0.8356\n",
      "Epoch 4/20\n",
      "1011/1011 [==============================] - 14s 14ms/step - loss: 2.3705 - accuracy: 0.8216 - val_loss: 0.4426 - val_accuracy: 0.8452\n",
      "Epoch 5/20\n",
      "1011/1011 [==============================] - 14s 14ms/step - loss: 2.2098 - accuracy: 0.8339 - val_loss: 0.4506 - val_accuracy: 0.8379\n",
      "Epoch 6/20\n",
      "1011/1011 [==============================] - 14s 14ms/step - loss: 2.1070 - accuracy: 0.8444 - val_loss: 0.4402 - val_accuracy: 0.8444\n",
      "Epoch 7/20\n",
      "1011/1011 [==============================] - 14s 14ms/step - loss: 1.9849 - accuracy: 0.8523 - val_loss: 0.4605 - val_accuracy: 0.8454\n",
      "Epoch 8/20\n",
      "1011/1011 [==============================] - 14s 14ms/step - loss: 1.8860 - accuracy: 0.8570 - val_loss: 0.4502 - val_accuracy: 0.8511\n",
      "Epoch 9/20\n",
      "1011/1011 [==============================] - 14s 14ms/step - loss: 1.7837 - accuracy: 0.8667 - val_loss: 0.4953 - val_accuracy: 0.8421\n",
      "241/241 [==============================] - 3s 11ms/step\n",
      "Fold 4: accuracy: 0.8443983402489627\n",
      "Fold 5: test fold: ['0008' '0011' '0015' '0020' '0007']\n",
      "Epoch 1/20\n",
      "1001/1001 [==============================] - 14s 14ms/step - loss: 3.8962 - accuracy: 0.7237 - val_loss: 0.5502 - val_accuracy: 0.8023\n",
      "Epoch 2/20\n",
      "1001/1001 [==============================] - 14s 14ms/step - loss: 2.7261 - accuracy: 0.7985 - val_loss: 0.4814 - val_accuracy: 0.8205\n",
      "Epoch 3/20\n",
      "1001/1001 [==============================] - 14s 14ms/step - loss: 2.3934 - accuracy: 0.8212 - val_loss: 0.4476 - val_accuracy: 0.8317\n",
      "Epoch 4/20\n",
      "1001/1001 [==============================] - 14s 14ms/step - loss: 2.1773 - accuracy: 0.8351 - val_loss: 0.4670 - val_accuracy: 0.8227\n",
      "Epoch 5/20\n",
      "1001/1001 [==============================] - 14s 14ms/step - loss: 2.0778 - accuracy: 0.8463 - val_loss: 0.4515 - val_accuracy: 0.8270\n",
      "Epoch 6/20\n",
      "1001/1001 [==============================] - 14s 14ms/step - loss: 1.8452 - accuracy: 0.8607 - val_loss: 0.4730 - val_accuracy: 0.8280\n",
      "Epoch 7/20\n",
      "1001/1001 [==============================] - 14s 14ms/step - loss: 1.7761 - accuracy: 0.8648 - val_loss: 0.4662 - val_accuracy: 0.8278\n",
      "Epoch 8/20\n",
      "1001/1001 [==============================] - 14s 14ms/step - loss: 1.6842 - accuracy: 0.8709 - val_loss: 0.4733 - val_accuracy: 0.8388\n",
      "250/250 [==============================] - 3s 11ms/step\n",
      "Fold 5: accuracy: 0.827\n",
      "Average Accuracy: 0.8318388022242857\n",
      "Average F1-Score: 0.8321140270388406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = SAT1Base(len(data.channels), len(data.samples), len(data.labels))\n",
    "model.compile(**get_compile_kwargs())\n",
    "train_kwargs = {\n",
    "    \"logs_path\": logs_path,\n",
    "    \"additional_info\": {\"preprocessing\": \"default_100hz\"},\n",
    "    \"additional_name\": f\"preprocessing-default_100hz\",\n",
    "}\n",
    "results = k_fold_cross_validate(\n",
    "    data, model, 5, normalization_fn=norm_dummy, train_kwargs=train_kwargs\n",
    ")\n",
    "print_results(results)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2b: Unprocessed 100Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/sat1/split_stage_data_unprocessed_100hz.nc\")\n",
    "data = xr.load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: test fold: ['0009' '0017' '0001' '0024' '0012']\n",
      "Epoch 1/20\n",
      "997/997 [==============================] - 15s 14ms/step - loss: 3.4801 - accuracy: 0.7509 - val_loss: 0.5346 - val_accuracy: 0.8065\n",
      "Epoch 2/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 2.6244 - accuracy: 0.8129 - val_loss: 0.5000 - val_accuracy: 0.8252\n",
      "Epoch 3/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 2.3060 - accuracy: 0.8306 - val_loss: 0.4662 - val_accuracy: 0.8384\n",
      "Epoch 4/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 2.1392 - accuracy: 0.8437 - val_loss: 0.4516 - val_accuracy: 0.8431\n",
      "Epoch 5/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 1.9021 - accuracy: 0.8601 - val_loss: 0.4549 - val_accuracy: 0.8386\n",
      "Epoch 6/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 1.7453 - accuracy: 0.8738 - val_loss: 0.4557 - val_accuracy: 0.8376\n",
      "Epoch 7/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 1.6421 - accuracy: 0.8776 - val_loss: 0.4490 - val_accuracy: 0.8533\n",
      "Epoch 8/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 1.4681 - accuracy: 0.8887 - val_loss: 0.4692 - val_accuracy: 0.8484\n",
      "Epoch 9/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 1.4217 - accuracy: 0.8989 - val_loss: 0.4579 - val_accuracy: 0.8491\n",
      "Epoch 10/20\n",
      "997/997 [==============================] - 14s 14ms/step - loss: 1.2412 - accuracy: 0.9077 - val_loss: 0.4572 - val_accuracy: 0.8446\n",
      "251/251 [==============================] - 3s 10ms/step\n",
      "Fold 1: accuracy: 0.8533366533864541\n",
      "Fold 2: test fold: ['0010' '0014' '0002' '0023' '0006']\n",
      "Epoch 1/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 3.9052 - accuracy: 0.7195 - val_loss: 0.6112 - val_accuracy: 0.7737\n",
      "Epoch 2/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 2.8202 - accuracy: 0.7913 - val_loss: 0.4972 - val_accuracy: 0.8225\n",
      "Epoch 3/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 2.4260 - accuracy: 0.8187 - val_loss: 0.4926 - val_accuracy: 0.8210\n",
      "Epoch 4/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 2.2892 - accuracy: 0.8258 - val_loss: 0.4808 - val_accuracy: 0.8271\n",
      "Epoch 5/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 2.0932 - accuracy: 0.8419 - val_loss: 0.4423 - val_accuracy: 0.8452\n",
      "Epoch 6/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 1.9960 - accuracy: 0.8514 - val_loss: 0.4419 - val_accuracy: 0.8459\n",
      "Epoch 7/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 1.8740 - accuracy: 0.8587 - val_loss: 0.4538 - val_accuracy: 0.8406\n",
      "Epoch 8/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 1.8103 - accuracy: 0.8638 - val_loss: 0.4482 - val_accuracy: 0.8501\n",
      "Epoch 9/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 1.5907 - accuracy: 0.8773 - val_loss: 0.4390 - val_accuracy: 0.8535\n",
      "Epoch 10/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 1.5033 - accuracy: 0.8858 - val_loss: 0.4365 - val_accuracy: 0.8577\n",
      "Epoch 11/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 1.4056 - accuracy: 0.8911 - val_loss: 0.4541 - val_accuracy: 0.8547\n",
      "Epoch 12/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 1.2970 - accuracy: 0.9003 - val_loss: 0.4965 - val_accuracy: 0.8542\n",
      "Epoch 13/20\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 1.2895 - accuracy: 0.9042 - val_loss: 0.5028 - val_accuracy: 0.8518\n",
      "256/256 [==============================] - 3s 10ms/step\n",
      "Fold 2: accuracy: 0.857666015625\n",
      "Fold 3: test fold: ['0003' '0013' '0016' '0004' '0005']\n",
      "Epoch 1/20\n",
      "999/999 [==============================] - 14s 14ms/step - loss: 3.5474 - accuracy: 0.7528 - val_loss: 0.5517 - val_accuracy: 0.7999\n",
      "Epoch 2/20\n",
      "999/999 [==============================] - 14s 14ms/step - loss: 2.6378 - accuracy: 0.8112 - val_loss: 0.5351 - val_accuracy: 0.8125\n",
      "Epoch 3/20\n",
      "999/999 [==============================] - 14s 14ms/step - loss: 2.3160 - accuracy: 0.8311 - val_loss: 0.4999 - val_accuracy: 0.8173\n",
      "Epoch 4/20\n",
      "999/999 [==============================] - 14s 14ms/step - loss: 2.1267 - accuracy: 0.8435 - val_loss: 0.4660 - val_accuracy: 0.8361\n",
      "Epoch 5/20\n",
      "999/999 [==============================] - 14s 14ms/step - loss: 2.0084 - accuracy: 0.8505 - val_loss: 0.4922 - val_accuracy: 0.8198\n",
      "Epoch 6/20\n",
      "999/999 [==============================] - 14s 14ms/step - loss: 1.8611 - accuracy: 0.8631 - val_loss: 0.4985 - val_accuracy: 0.8233\n",
      "Epoch 7/20\n",
      "999/999 [==============================] - 14s 14ms/step - loss: 1.7578 - accuracy: 0.8693 - val_loss: 0.4891 - val_accuracy: 0.8296\n",
      "249/249 [==============================] - 3s 11ms/step\n",
      "Fold 3: accuracy: 0.8360943775100401\n",
      "Fold 4: test fold: ['0021' '0018' '0022' '0019' '0025']\n",
      "Epoch 1/20\n",
      "1008/1008 [==============================] - 14s 14ms/step - loss: 3.7614 - accuracy: 0.7405 - val_loss: 0.4651 - val_accuracy: 0.8327\n",
      "Epoch 2/20\n",
      "1008/1008 [==============================] - 14s 14ms/step - loss: 2.6389 - accuracy: 0.8047 - val_loss: 0.5074 - val_accuracy: 0.8195\n",
      "Epoch 3/20\n",
      "1008/1008 [==============================] - 14s 14ms/step - loss: 2.4270 - accuracy: 0.8218 - val_loss: 0.4696 - val_accuracy: 0.8332\n",
      "Epoch 4/20\n",
      "1008/1008 [==============================] - 14s 14ms/step - loss: 2.2047 - accuracy: 0.8359 - val_loss: 0.4226 - val_accuracy: 0.8587\n",
      "Epoch 5/20\n",
      "1008/1008 [==============================] - 14s 14ms/step - loss: 2.0879 - accuracy: 0.8447 - val_loss: 0.4064 - val_accuracy: 0.8527\n",
      "Epoch 6/20\n",
      "1008/1008 [==============================] - 14s 14ms/step - loss: 2.0028 - accuracy: 0.8467 - val_loss: 0.4153 - val_accuracy: 0.8576\n",
      "Epoch 7/20\n",
      "1008/1008 [==============================] - 14s 14ms/step - loss: 1.8328 - accuracy: 0.8609 - val_loss: 0.4278 - val_accuracy: 0.8641\n",
      "Epoch 8/20\n",
      "1008/1008 [==============================] - 14s 14ms/step - loss: 1.7824 - accuracy: 0.8643 - val_loss: 0.4638 - val_accuracy: 0.8426\n",
      "241/241 [==============================] - 3s 11ms/step\n",
      "Fold 4: accuracy: 0.8526970954356846\n",
      "Fold 5: test fold: ['0008' '0011' '0015' '0020' '0007']\n",
      "Epoch 1/20\n",
      "998/998 [==============================] - 14s 14ms/step - loss: 3.4941 - accuracy: 0.7578 - val_loss: 0.5525 - val_accuracy: 0.7937\n",
      "Epoch 2/20\n",
      "998/998 [==============================] - 14s 14ms/step - loss: 2.5227 - accuracy: 0.8165 - val_loss: 0.5137 - val_accuracy: 0.8117\n",
      "Epoch 3/20\n",
      "998/998 [==============================] - 14s 14ms/step - loss: 2.2106 - accuracy: 0.8349 - val_loss: 0.4971 - val_accuracy: 0.8167\n",
      "Epoch 4/20\n",
      "998/998 [==============================] - 14s 14ms/step - loss: 2.0230 - accuracy: 0.8526 - val_loss: 0.4871 - val_accuracy: 0.8255\n",
      "Epoch 5/20\n",
      "998/998 [==============================] - 14s 14ms/step - loss: 1.8643 - accuracy: 0.8618 - val_loss: 0.5087 - val_accuracy: 0.8207\n",
      "975/998 [============================>.] - ETA: 0s - loss: 1.8593 - accuracy: 0.8617Epoch 6/20\n",
      "998/998 [==============================] - 14s 14ms/step - loss: 1.7266 - accuracy: 0.8719 - val_loss: 0.5067 - val_accuracy: 0.8213\n",
      "Epoch 7/20\n",
      "998/998 [==============================] - 14s 14ms/step - loss: 1.5975 - accuracy: 0.8813 - val_loss: 0.5027 - val_accuracy: 0.8310\n",
      "250/250 [==============================] - 3s 11ms/step\n",
      "Fold 5: accuracy: 0.8255\n",
      "Average Accuracy: 0.8450588283914359\n",
      "Average F1-Score: 0.8452467751439409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38733"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = SAT1Base(len(data.channels), len(data.samples), len(data.labels))\n",
    "model.compile(**get_compile_kwargs())\n",
    "train_kwargs = {\n",
    "    \"logs_path\": logs_path,\n",
    "    \"additional_info\": {\"preprocessing\": \"unprocessed_100hz\"},\n",
    "    \"additional_name\": f\"preprocessing-unprocessed_100hz\",\n",
    "}\n",
    "results = k_fold_cross_validate(\n",
    "    data, model, 5, normalization_fn=norm_dummy, train_kwargs=train_kwargs\n",
    ")\n",
    "print_results(results)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2c: Unprocessed 500Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/sat1/split_stage_data_unprocessed_500hz.nc\")\n",
    "data = xr.load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 12:47:31.599145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-20 12:47:31.656433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-20 12:47:31.656695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-20 12:47:31.663288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-20 12:47:31.663587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-20 12:47:31.663788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-20 12:47:32.389599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-20 12:47:32.389708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-20 12:47:32.389718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-20 12:47:32.389786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-20 12:47:32.389798: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-10-20 12:47:32.389804: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2023-10-20 12:47:32.389936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21598 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: test fold: ['0009' '0017' '0001' '0024' '0012']\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 12:47:38.888245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-10-20 12:47:40.188212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-10-20 12:47:41.504893: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe3ea38c3c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-20 12:47:41.504925: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-10-20 12:47:41.510793: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-20 12:47:41.618232: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 62s 56ms/step - loss: 3.1762 - accuracy: 0.7808 - val_loss: 0.8299 - val_accuracy: 0.7642\n",
      "Epoch 2/20\n",
      "993/993 [==============================] - 56s 56ms/step - loss: 2.4940 - accuracy: 0.8145 - val_loss: 0.4755 - val_accuracy: 0.8127\n",
      "Epoch 3/20\n",
      "993/993 [==============================] - 56s 56ms/step - loss: 2.1913 - accuracy: 0.8328 - val_loss: 0.4331 - val_accuracy: 0.8265\n",
      "Epoch 4/20\n",
      "993/993 [==============================] - 57s 57ms/step - loss: 2.0220 - accuracy: 0.8481 - val_loss: 0.4324 - val_accuracy: 0.8335\n",
      "Epoch 5/20\n",
      "993/993 [==============================] - 56s 56ms/step - loss: 1.8801 - accuracy: 0.8568 - val_loss: 0.4289 - val_accuracy: 0.8202\n",
      "Epoch 6/20\n",
      "993/993 [==============================] - 56s 56ms/step - loss: 1.7325 - accuracy: 0.8662 - val_loss: 0.4036 - val_accuracy: 0.8355\n",
      "Epoch 7/20\n",
      "993/993 [==============================] - 56s 56ms/step - loss: 1.6484 - accuracy: 0.8732 - val_loss: 0.3918 - val_accuracy: 0.8395\n",
      "Epoch 8/20\n",
      "993/993 [==============================] - 56s 56ms/step - loss: 1.4569 - accuracy: 0.8873 - val_loss: 0.3821 - val_accuracy: 0.8435\n",
      "Epoch 9/20\n",
      "993/993 [==============================] - 56s 56ms/step - loss: 1.4227 - accuracy: 0.8927 - val_loss: 0.3907 - val_accuracy: 0.8487\n",
      "Epoch 10/20\n",
      "993/993 [==============================] - 56s 56ms/step - loss: 1.2320 - accuracy: 0.9054 - val_loss: 0.4046 - val_accuracy: 0.8500\n",
      "Epoch 11/20\n",
      "993/993 [==============================] - 57s 57ms/step - loss: 1.0467 - accuracy: 0.9201 - val_loss: 0.4237 - val_accuracy: 0.8478\n",
      "250/250 [==============================] - 3s 13ms/step\n",
      "Fold 1: accuracy: 0.8435\n",
      "Fold 2: test fold: ['0010' '0014' '0002' '0023' '0006']\n",
      "Epoch 1/20\n",
      "988/988 [==============================] - 58s 56ms/step - loss: 3.1440 - accuracy: 0.7749 - val_loss: 0.4770 - val_accuracy: 0.8120\n",
      "Epoch 2/20\n",
      "988/988 [==============================] - 56s 56ms/step - loss: 2.5335 - accuracy: 0.8110 - val_loss: 0.4702 - val_accuracy: 0.8162\n",
      "Epoch 3/20\n",
      "988/988 [==============================] - 56s 56ms/step - loss: 2.2461 - accuracy: 0.8309 - val_loss: 0.4421 - val_accuracy: 0.8105\n",
      "Epoch 4/20\n",
      "988/988 [==============================] - 57s 57ms/step - loss: 2.0735 - accuracy: 0.8410 - val_loss: 0.3851 - val_accuracy: 0.8493\n",
      "Epoch 5/20\n",
      "988/988 [==============================] - 56s 57ms/step - loss: 1.9172 - accuracy: 0.8558 - val_loss: 0.3760 - val_accuracy: 0.8561\n",
      "Epoch 6/20\n",
      "988/988 [==============================] - 56s 56ms/step - loss: 1.8324 - accuracy: 0.8553 - val_loss: 0.3844 - val_accuracy: 0.8434\n",
      "Epoch 7/20\n",
      "988/988 [==============================] - 56s 56ms/step - loss: 1.6854 - accuracy: 0.8699 - val_loss: 0.3765 - val_accuracy: 0.8517\n",
      "Epoch 8/20\n",
      "988/988 [==============================] - 57s 57ms/step - loss: 1.5835 - accuracy: 0.8777 - val_loss: 0.3838 - val_accuracy: 0.8507\n",
      "255/255 [==============================] - 3s 13ms/step\n",
      "Fold 2: accuracy: 0.8561274509803921\n",
      "Fold 3: test fold: ['0003' '0013' '0016' '0004' '0005']\n",
      "Epoch 1/20\n",
      "995/995 [==============================] - 58s 56ms/step - loss: 3.1655 - accuracy: 0.7737 - val_loss: 0.4579 - val_accuracy: 0.8274\n",
      "Epoch 2/20\n",
      "995/995 [==============================] - 56s 56ms/step - loss: 2.4268 - accuracy: 0.8210 - val_loss: 0.4550 - val_accuracy: 0.8085\n",
      "Epoch 3/20\n",
      "995/995 [==============================] - 56s 56ms/step - loss: 2.1877 - accuracy: 0.8343 - val_loss: 0.3941 - val_accuracy: 0.8526\n",
      "Epoch 4/20\n",
      "995/995 [==============================] - 57s 57ms/step - loss: 2.0215 - accuracy: 0.8462 - val_loss: 0.3882 - val_accuracy: 0.8488\n",
      "Epoch 5/20\n",
      "995/995 [==============================] - 57s 57ms/step - loss: 1.8613 - accuracy: 0.8575 - val_loss: 0.3802 - val_accuracy: 0.8564\n",
      "Epoch 6/20\n",
      "995/995 [==============================] - 56s 56ms/step - loss: 1.7089 - accuracy: 0.8699 - val_loss: 0.3933 - val_accuracy: 0.8541\n",
      "Epoch 7/20\n",
      "995/995 [==============================] - 56s 56ms/step - loss: 1.6286 - accuracy: 0.8776 - val_loss: 0.3956 - val_accuracy: 0.8463\n",
      "Epoch 8/20\n",
      "995/995 [==============================] - 57s 57ms/step - loss: 1.4455 - accuracy: 0.8876 - val_loss: 0.3994 - val_accuracy: 0.8627\n",
      "248/248 [==============================] - 3s 13ms/step\n",
      "Fold 3: accuracy: 0.8563508064516129\n",
      "Fold 4: test fold: ['0021' '0018' '0022' '0019' '0025']\n",
      "Epoch 1/20\n",
      "1004/1004 [==============================] - 70s 68ms/step - loss: 3.2507 - accuracy: 0.7675 - val_loss: 0.4339 - val_accuracy: 0.8329\n",
      "Epoch 2/20\n",
      "1004/1004 [==============================] - 57s 56ms/step - loss: 2.5797 - accuracy: 0.8050 - val_loss: 0.3707 - val_accuracy: 0.8601\n",
      "Epoch 3/20\n",
      "1004/1004 [==============================] - 57s 56ms/step - loss: 2.3319 - accuracy: 0.8214 - val_loss: 0.3835 - val_accuracy: 0.8502\n",
      "Epoch 4/20\n",
      "1004/1004 [==============================] - 61s 60ms/step - loss: 2.2494 - accuracy: 0.8325 - val_loss: 0.3696 - val_accuracy: 0.8509\n",
      "Epoch 5/20\n",
      "1004/1004 [==============================] - 58s 57ms/step - loss: 2.0394 - accuracy: 0.8435 - val_loss: 0.3547 - val_accuracy: 0.8706\n",
      "Epoch 6/20\n",
      "1004/1004 [==============================] - 58s 57ms/step - loss: 1.9061 - accuracy: 0.8541 - val_loss: 0.3373 - val_accuracy: 0.8666\n",
      "Epoch 7/20\n",
      "1004/1004 [==============================] - 57s 56ms/step - loss: 1.7639 - accuracy: 0.8625 - val_loss: 0.3753 - val_accuracy: 0.8606\n",
      "Epoch 8/20\n",
      "1004/1004 [==============================] - 57s 56ms/step - loss: 1.6221 - accuracy: 0.8734 - val_loss: 0.3474 - val_accuracy: 0.8700\n",
      "Epoch 9/20\n",
      "1004/1004 [==============================] - 58s 57ms/step - loss: 1.4867 - accuracy: 0.8843 - val_loss: 0.4070 - val_accuracy: 0.8661\n",
      "239/239 [==============================] - 3s 13ms/step\n",
      "Fold 4: accuracy: 0.8666317991631799\n",
      "Fold 5: test fold: ['0008' '0011' '0015' '0020' '0007']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = SAT1Deep(len(data.channels), len(data.samples), len(data.labels))\n",
    "# model.compile(**get_compile_kwargs())\n",
    "train_kwargs = {\n",
    "    \"logs_path\": logs_path,\n",
    "    \"additional_info\": {\"preprocessing\": \"unprocessed_500hz\"},\n",
    "    \"additional_name\": f\"preprocessing-unprocessed_500hz\",\n",
    "}\n",
    "results = k_fold_cross_validate(\n",
    "    data, model, 5, normalization_fn=norm_dummy, train_kwargs=train_kwargs\n",
    ")\n",
    "print_results(results)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-25 13:59:04.226414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.13.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# View results in Tensorboard\n",
    "! tensorboard --logdir logs/exp_preprocessing/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
