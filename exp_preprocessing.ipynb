{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Pre-processing and frequency sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Is it possible to train a model on unpreprocessed EEG data and still attain similar performance levels?\n",
    "\n",
    "**Hypothesis**: The model will perform worse, but if still similar then the added value of not having to (manually) preprocess EEG data is very valuable and opens up a multitude of applications.\n",
    "\n",
    "**Result**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Preparing data\n",
    "To use hmp.utils.read_mne_data() and epoch the information, the files should be in .fif format, this replicates automated preprocessing as done in https://github.com/GWeindel/hsmm_mvpy/blob/main/tutorials/sample_data/eeg/0022.ipynb excepting resampling to 100Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 16:27:02.985116: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 16:27:03.584778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "from pathlib import Path\n",
    "import hsmm_mvpy as hmp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from shared.data import add_stage_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths and file locations\n",
    "data_path = Path(\"/mnt/d/thesis/sat1/\")\n",
    "behavioral_data_path = data_path / \"ExperimentData/ExperimentData\"\n",
    "output_path = Path(\"data/sat1/unpreprocessed\")\n",
    "\n",
    "subj_ids = [\n",
    "    subj_id.name.split(\"-\")[1][:4] for subj_id in (data_path / \"eeg4\").glob(\"*.vhdr\")\n",
    "]\n",
    "subj_files = [\n",
    "    str(output_path / f\"unprocessed_{subj_id}_epo.fif\") for subj_id in subj_ids\n",
    "]\n",
    "behavioral_files = [\n",
    "    str(behavioral_data_path / f\"{subj_id}-cnv-sat3_ET.csv\") for subj_id in subj_ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing preprocessing done in https://github.com/GWeindel/hsmm_mvpy/blob/main/tutorials/sample_data/eeg/0022.ipynb\n",
    "# with only the necessary (non-manual) parts, like adding metadata for processing in HMP package, more info in link above\n",
    "for subject_id in subj_ids:\n",
    "    print(f\"Processing subject: {subject_id}\")\n",
    "    subject_id_short = subject_id.replace(\"0\", \"\")\n",
    "    raw = mne.io.read_raw_brainvision(\n",
    "        data_path / \"eeg4\" / f\"MD3-{subject_id}.vhdr\", preload=False\n",
    "    )\n",
    "    raw.set_channel_types(\n",
    "        {\"EOGh\": \"eog\", \"EOGv\": \"eog\", \"A1\": \"misc\", \"A2\": \"misc\"}\n",
    "    )  # Declare type to avoid confusion with EEG channels\n",
    "    raw.rename_channels({\"FP1\": \"Fp1\", \"FP2\": \"Fp2\"})  # Naming convention\n",
    "    raw.set_montage(\"standard_1020\")  # Standard 10-20 electrode montage\n",
    "    raw.rename_channels({\"Fp1\": \"FP1\", \"Fp2\": \"FP2\"})\n",
    "\n",
    "    behavioral_path = behavioral_data_path / f\"{subject_id}-cnv-sat3_ET.csv\"\n",
    "    behavior = pd.read_csv(behavioral_path, sep=\";\")[\n",
    "        [\n",
    "            \"stim\",\n",
    "            \"resp\",\n",
    "            \"RT\",\n",
    "            \"cue\",\n",
    "            \"movement\",\n",
    "        ]\n",
    "    ]\n",
    "    behavior[\"movement\"] = behavior.apply(\n",
    "        lambda row: \"stim_left\"\n",
    "        if row[\"movement\"] == -1\n",
    "        else (\"stim_right\" if row[\"movement\"] == 1 else np.nan),\n",
    "        axis=1,\n",
    "    )\n",
    "    behavior[\"resp\"] = behavior.apply(\n",
    "        lambda row: \"resp_left\"\n",
    "        if row[\"resp\"] == 1\n",
    "        else (\"resp_right\" if row[\"resp\"] == 2 else np.nan),\n",
    "        axis=1,\n",
    "    )\n",
    "    # Merging together the exeperimental conditions info to have the format condition/stimulus/response\n",
    "    behavior[\"trigger\"] = (\n",
    "        behavior[\"cue\"] + \"/\" + behavior[\"movement\"] + \"/\" + behavior[\"resp\"]\n",
    "    )\n",
    "    # Filtering out < 300 and > 3000 Reaction times\n",
    "    behavior[\"RT\"] = behavior.apply(\n",
    "        lambda row: 0\n",
    "        if row[\"RT\"] < 300\n",
    "        else (0 if row[\"RT\"] > 3000 else float(row[\"RT\"]) / 1000),\n",
    "        axis=1,\n",
    "    )\n",
    "    epochs = mne.io.read_epochs_fieldtrip(\n",
    "        data_path / \"eeg1\" / f\"data{subject_id_short}.mat\", info=raw.info\n",
    "    )\n",
    "    epochs.rename_channels({\"FP1\": \"Fp1\", \"FP2\": \"Fp2\"})  # Naming convention\n",
    "    epochs.set_montage(\"easycap-M1\")\n",
    "    epochs.filter(1, 35)  # Bandwidth filter from van Maanen, Portoles & Borst (2021)\n",
    "    epochs.crop(tmin=-0.250)\n",
    "    epochs.set_eeg_reference(\"average\")\n",
    "    epochs.metadata = behavior\n",
    "    epochs.save(\n",
    "        output_path / f\"unprocessed_{subject_id}_epo.fif\", overwrite=True, verbose=False\n",
    "    )  # Saving EEG mne format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing participant data/sat1/unpreprocessed/unprocessed_0001_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0001_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0002_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0002_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0003_epo.fif's epoched eeg\n",
      "191 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0003_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0004_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0004_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0005_epo.fif's epoched eeg\n",
      "190 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0005_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0006_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0006_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0007_epo.fif's epoched eeg\n",
      "199 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0007_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0008_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0008_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0009_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0009_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0010_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0010_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0011_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0011_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0012_epo.fif's epoched eeg\n",
      "188 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0012_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0013_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0013_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0014_epo.fif's epoched eeg\n",
      "186 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0014_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0015_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0015_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0016_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0016_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0017_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0017_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0018_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0018_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0019_epo.fif's epoched eeg\n",
      "197 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0019_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0020_epo.fif's epoched eeg\n",
      "178 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0020_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0021_epo.fif's epoched eeg\n",
      "199 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0021_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0022_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0022_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0023_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0023_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0024_epo.fif's epoched eeg\n",
      "185 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0024_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0025_epo.fif's epoched eeg\n",
      "199 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0025_epo.fif\n"
     ]
    }
   ],
   "source": [
    "output_path_data = Path(\"data/sat1/data_unprocessed_500hz.nc\")\n",
    "# Run if data_unprocessed.nc does not exist or should be rewritten\n",
    "data = hmp.utils.read_mne_data(\n",
    "    subj_files,\n",
    "    epoched=True,\n",
    "    lower_limit_RT=0.2,\n",
    "    upper_limit_RT=2,\n",
    "    verbose=False,\n",
    "    subj_idx=subj_ids,\n",
    "    rt_col=\"RT\",\n",
    ")\n",
    "data.to_netcdf(output_path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing participant data/sat1/unpreprocessed/unprocessed_0001_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0001_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0002_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0002_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0003_epo.fif's epoched eeg\n",
      "191 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0003_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0004_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0004_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0005_epo.fif's epoched eeg\n",
      "190 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0005_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0006_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0006_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0007_epo.fif's epoched eeg\n",
      "199 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0007_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0008_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0008_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0009_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0009_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0010_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0010_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0011_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0011_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0012_epo.fif's epoched eeg\n",
      "188 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0012_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0013_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0013_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0014_epo.fif's epoched eeg\n",
      "186 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0014_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0015_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0015_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0016_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0016_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0017_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0017_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0018_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0018_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0019_epo.fif's epoched eeg\n",
      "197 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0019_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0020_epo.fif's epoched eeg\n",
      "178 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0020_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0021_epo.fif's epoched eeg\n",
      "199 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0021_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0022_epo.fif's epoched eeg\n",
      "200 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0022_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0023_epo.fif's epoched eeg\n",
      "198 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0023_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0024_epo.fif's epoched eeg\n",
      "185 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0024_epo.fif\n",
      "Processing participant data/sat1/unpreprocessed/unprocessed_0025_epo.fif's epoched eeg\n",
      "199 trials were retained for participant data/sat1/unpreprocessed/unprocessed_0025_epo.fif\n"
     ]
    }
   ],
   "source": [
    "output_path_data = Path(\"data/sat1/data_unprocessed_100hz.nc\")\n",
    "# Run if data_unprocessed.nc does not exist or should be rewritten\n",
    "data = hmp.utils.read_mne_data(\n",
    "    subj_files,\n",
    "    epoched=True,\n",
    "    lower_limit_RT=0.2,\n",
    "    upper_limit_RT=2,\n",
    "    sfreq=100,\n",
    "    verbose=False,\n",
    "    subj_idx=subj_ids,\n",
    "    rt_col=\"RT\",\n",
    ")\n",
    "data.to_netcdf(output_path_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use information from stage_data to split unprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 500Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding stage changes\n",
      "17 0 response\n",
      "<xarray.DataArray 'labels' (labels: 1)>\n",
      "array(['response'], dtype='<U8')\n",
      "Coordinates:\n",
      "  * labels   (labels) <U8 'response'\n",
      "23\n",
      "8\n",
      "\n",
      "21 28 response\n",
      "<xarray.DataArray 'labels' (labels: 1)>\n",
      "array(['response'], dtype='<U8')\n",
      "Coordinates:\n",
      "  * labels   (labels) <U8 'response'\n",
      "53\n",
      "13\n",
      "\n",
      "Combining segments\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/git/hmp-ai/exp_preprocessing.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data_path \u001b[39m=\u001b[39m Path(\u001b[39m\"\u001b[39m\u001b[39mdata/sat1/stage_data.nc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m merge_dataset \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39mload_dataset(Path(\u001b[39m\"\u001b[39m\u001b[39mdata/sat1/data_unprocessed_500hz.nc\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m output_data \u001b[39m=\u001b[39m add_stage_dimension(data_path, merge_dataset)\n",
      "File \u001b[0;32m/mnt/c/git/hmp-ai/shared/data.py:129\u001b[0m, in \u001b[0;36madd_stage_dimension\u001b[0;34m(data_path, merge_dataset)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m# Recombine into new segments dimension\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCombining segments\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m combined_segments \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39;49mcombine_by_coords(segments)\n\u001b[1;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m combined_segments\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/combine.py:975\u001b[0m, in \u001b[0;36mcombine_by_coords\u001b[0;34m(data_objects, compat, data_vars, coords, fill_value, join, combine_attrs, datasets)\u001b[0m\n\u001b[1;32m    973\u001b[0m     concatenated_grouped_by_data_vars \u001b[39m=\u001b[39m []\n\u001b[1;32m    974\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39mvars\u001b[39m, datasets_with_same_vars \u001b[39min\u001b[39;00m grouped_by_vars:\n\u001b[0;32m--> 975\u001b[0m         concatenated \u001b[39m=\u001b[39m _combine_single_variable_hypercube(\n\u001b[1;32m    976\u001b[0m             \u001b[39mlist\u001b[39;49m(datasets_with_same_vars),\n\u001b[1;32m    977\u001b[0m             fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[1;32m    978\u001b[0m             data_vars\u001b[39m=\u001b[39;49mdata_vars,\n\u001b[1;32m    979\u001b[0m             coords\u001b[39m=\u001b[39;49mcoords,\n\u001b[1;32m    980\u001b[0m             compat\u001b[39m=\u001b[39;49mcompat,\n\u001b[1;32m    981\u001b[0m             join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    982\u001b[0m             combine_attrs\u001b[39m=\u001b[39;49mcombine_attrs,\n\u001b[1;32m    983\u001b[0m         )\n\u001b[1;32m    984\u001b[0m         concatenated_grouped_by_data_vars\u001b[39m.\u001b[39mappend(concatenated)\n\u001b[1;32m    986\u001b[0m \u001b[39mreturn\u001b[39;00m merge(\n\u001b[1;32m    987\u001b[0m     concatenated_grouped_by_data_vars,\n\u001b[1;32m    988\u001b[0m     compat\u001b[39m=\u001b[39mcompat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    991\u001b[0m     combine_attrs\u001b[39m=\u001b[39mcombine_attrs,\n\u001b[1;32m    992\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/combine.py:633\u001b[0m, in \u001b[0;36m_combine_single_variable_hypercube\u001b[0;34m(datasets, fill_value, data_vars, coords, compat, join, combine_attrs)\u001b[0m\n\u001b[1;32m    630\u001b[0m     _check_dimension_depth_tile_ids(combined_ids)\n\u001b[1;32m    632\u001b[0m \u001b[39m# Concatenate along all of concat_dims one by one to create single ds\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m concatenated \u001b[39m=\u001b[39m _combine_nd(\n\u001b[1;32m    634\u001b[0m     combined_ids,\n\u001b[1;32m    635\u001b[0m     concat_dims\u001b[39m=\u001b[39;49mconcat_dims,\n\u001b[1;32m    636\u001b[0m     data_vars\u001b[39m=\u001b[39;49mdata_vars,\n\u001b[1;32m    637\u001b[0m     coords\u001b[39m=\u001b[39;49mcoords,\n\u001b[1;32m    638\u001b[0m     compat\u001b[39m=\u001b[39;49mcompat,\n\u001b[1;32m    639\u001b[0m     fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[1;32m    640\u001b[0m     join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    641\u001b[0m     combine_attrs\u001b[39m=\u001b[39;49mcombine_attrs,\n\u001b[1;32m    642\u001b[0m )\n\u001b[1;32m    644\u001b[0m \u001b[39m# Check the overall coordinates are monotonically increasing\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m concat_dims:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/combine.py:235\u001b[0m, in \u001b[0;36m_combine_nd\u001b[0;34m(combined_ids, concat_dims, data_vars, coords, compat, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39m# Each iteration of this loop reduces the length of the tile_ids tuples\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# by one. It always combines along the first dimension, removing the first\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39m# element of the tuple\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39mfor\u001b[39;00m concat_dim \u001b[39min\u001b[39;00m concat_dims:\n\u001b[0;32m--> 235\u001b[0m     combined_ids \u001b[39m=\u001b[39m _combine_all_along_first_dim(\n\u001b[1;32m    236\u001b[0m         combined_ids,\n\u001b[1;32m    237\u001b[0m         dim\u001b[39m=\u001b[39;49mconcat_dim,\n\u001b[1;32m    238\u001b[0m         data_vars\u001b[39m=\u001b[39;49mdata_vars,\n\u001b[1;32m    239\u001b[0m         coords\u001b[39m=\u001b[39;49mcoords,\n\u001b[1;32m    240\u001b[0m         compat\u001b[39m=\u001b[39;49mcompat,\n\u001b[1;32m    241\u001b[0m         fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[1;32m    242\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    243\u001b[0m         combine_attrs\u001b[39m=\u001b[39;49mcombine_attrs,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m (combined_ds,) \u001b[39m=\u001b[39m combined_ids\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    246\u001b[0m \u001b[39mreturn\u001b[39;00m combined_ds\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/combine.py:270\u001b[0m, in \u001b[0;36m_combine_all_along_first_dim\u001b[0;34m(combined_ids, dim, data_vars, coords, compat, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    268\u001b[0m     combined_ids \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39msorted\u001b[39m(group))\n\u001b[1;32m    269\u001b[0m     datasets \u001b[39m=\u001b[39m combined_ids\u001b[39m.\u001b[39mvalues()\n\u001b[0;32m--> 270\u001b[0m     new_combined_ids[new_id] \u001b[39m=\u001b[39m _combine_1d(\n\u001b[1;32m    271\u001b[0m         datasets, dim, compat, data_vars, coords, fill_value, join, combine_attrs\n\u001b[1;32m    272\u001b[0m     )\n\u001b[1;32m    273\u001b[0m \u001b[39mreturn\u001b[39;00m new_combined_ids\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/combine.py:293\u001b[0m, in \u001b[0;36m_combine_1d\u001b[0;34m(datasets, concat_dim, compat, data_vars, coords, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mif\u001b[39;00m concat_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 293\u001b[0m         combined \u001b[39m=\u001b[39m concat(\n\u001b[1;32m    294\u001b[0m             datasets,\n\u001b[1;32m    295\u001b[0m             dim\u001b[39m=\u001b[39;49mconcat_dim,\n\u001b[1;32m    296\u001b[0m             data_vars\u001b[39m=\u001b[39;49mdata_vars,\n\u001b[1;32m    297\u001b[0m             coords\u001b[39m=\u001b[39;49mcoords,\n\u001b[1;32m    298\u001b[0m             compat\u001b[39m=\u001b[39;49mcompat,\n\u001b[1;32m    299\u001b[0m             fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[1;32m    300\u001b[0m             join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    301\u001b[0m             combine_attrs\u001b[39m=\u001b[39;49mcombine_attrs,\n\u001b[1;32m    302\u001b[0m         )\n\u001b[1;32m    303\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    304\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencountered unexpected variable\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/concat.py:251\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m _dataarray_concat(\n\u001b[1;32m    240\u001b[0m         objs,\n\u001b[1;32m    241\u001b[0m         dim\u001b[39m=\u001b[39mdim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m         combine_attrs\u001b[39m=\u001b[39mcombine_attrs,\n\u001b[1;32m    249\u001b[0m     )\n\u001b[1;32m    250\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(first_obj, Dataset):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mreturn\u001b[39;00m _dataset_concat(\n\u001b[1;32m    252\u001b[0m         objs,\n\u001b[1;32m    253\u001b[0m         dim\u001b[39m=\u001b[39;49mdim,\n\u001b[1;32m    254\u001b[0m         data_vars\u001b[39m=\u001b[39;49mdata_vars,\n\u001b[1;32m    255\u001b[0m         coords\u001b[39m=\u001b[39;49mcoords,\n\u001b[1;32m    256\u001b[0m         compat\u001b[39m=\u001b[39;49mcompat,\n\u001b[1;32m    257\u001b[0m         positions\u001b[39m=\u001b[39;49mpositions,\n\u001b[1;32m    258\u001b[0m         fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[1;32m    259\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    260\u001b[0m         combine_attrs\u001b[39m=\u001b[39;49mcombine_attrs,\n\u001b[1;32m    261\u001b[0m     )\n\u001b[1;32m    262\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcan only concatenate xarray Dataset and DataArray \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mobjects, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(first_obj)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/concat.py:483\u001b[0m, in \u001b[0;36m_dataset_concat\u001b[0;34m(datasets, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39m# Make sure we're working on a copy (we'll be loading variables)\u001b[39;00m\n\u001b[1;32m    481\u001b[0m datasets \u001b[39m=\u001b[39m [ds\u001b[39m.\u001b[39mcopy() \u001b[39mfor\u001b[39;00m ds \u001b[39min\u001b[39;00m datasets]\n\u001b[1;32m    482\u001b[0m datasets \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m--> 483\u001b[0m     align(\u001b[39m*\u001b[39;49mdatasets, join\u001b[39m=\u001b[39;49mjoin, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, exclude\u001b[39m=\u001b[39;49m[dim], fill_value\u001b[39m=\u001b[39;49mfill_value)\n\u001b[1;32m    484\u001b[0m )\n\u001b[1;32m    486\u001b[0m dim_coords, dims_sizes, coord_names, data_names, vars_order \u001b[39m=\u001b[39m _parse_datasets(\n\u001b[1;32m    487\u001b[0m     datasets\n\u001b[1;32m    488\u001b[0m )\n\u001b[1;32m    489\u001b[0m dim_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(dim_coords)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/alignment.py:787\u001b[0m, in \u001b[0;36malign\u001b[0;34m(join, copy, indexes, exclude, fill_value, *objects)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39mGiven any number of Dataset and/or DataArray objects, returns new\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[39mobjects with aligned indexes and dimension sizes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    777\u001b[0m \n\u001b[1;32m    778\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    779\u001b[0m aligner \u001b[39m=\u001b[39m Aligner(\n\u001b[1;32m    780\u001b[0m     objects,\n\u001b[1;32m    781\u001b[0m     join\u001b[39m=\u001b[39mjoin,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    785\u001b[0m     fill_value\u001b[39m=\u001b[39mfill_value,\n\u001b[1;32m    786\u001b[0m )\n\u001b[0;32m--> 787\u001b[0m aligner\u001b[39m.\u001b[39;49malign()\n\u001b[1;32m    788\u001b[0m \u001b[39mreturn\u001b[39;00m aligner\u001b[39m.\u001b[39mresults\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/alignment.py:580\u001b[0m, in \u001b[0;36mAligner.align\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjects\n\u001b[1;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreindex_all()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/alignment.py:555\u001b[0m, in \u001b[0;36mAligner.reindex_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreindex_all\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m    556\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_one(obj, matching_indexes)\n\u001b[1;32m    557\u001b[0m         \u001b[39mfor\u001b[39;00m obj, matching_indexes \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    558\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjects, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjects_matching_indexes\n\u001b[1;32m    559\u001b[0m         )\n\u001b[1;32m    560\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/alignment.py:556\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreindex_all\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    555\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[0;32m--> 556\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reindex_one(obj, matching_indexes)\n\u001b[1;32m    557\u001b[0m         \u001b[39mfor\u001b[39;00m obj, matching_indexes \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    558\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjects, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjects_matching_indexes\n\u001b[1;32m    559\u001b[0m         )\n\u001b[1;32m    560\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/alignment.py:542\u001b[0m, in \u001b[0;36mAligner._reindex_one\u001b[0;34m(self, obj, matching_indexes)\u001b[0m\n\u001b[1;32m    539\u001b[0m new_indexes, new_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_indexes_and_vars(obj, matching_indexes)\n\u001b[1;32m    540\u001b[0m dim_pos_indexers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dim_pos_indexers(matching_indexes)\n\u001b[0;32m--> 542\u001b[0m new_obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_reindex_callback(\n\u001b[1;32m    543\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    544\u001b[0m     dim_pos_indexers,\n\u001b[1;32m    545\u001b[0m     new_variables,\n\u001b[1;32m    546\u001b[0m     new_indexes,\n\u001b[1;32m    547\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfill_value,\n\u001b[1;32m    548\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexclude_dims,\n\u001b[1;32m    549\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexclude_vars,\n\u001b[1;32m    550\u001b[0m )\n\u001b[1;32m    551\u001b[0m new_obj\u001b[39m.\u001b[39mencoding \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mencoding\n\u001b[1;32m    552\u001b[0m \u001b[39mreturn\u001b[39;00m new_obj\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/dataset.py:2911\u001b[0m, in \u001b[0;36mDataset._reindex_callback\u001b[0;34m(self, aligner, dim_pos_indexers, variables, indexes, fill_value, exclude_dims, exclude_vars)\u001b[0m\n\u001b[1;32m   2905\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2906\u001b[0m     to_reindex \u001b[39m=\u001b[39m {\n\u001b[1;32m   2907\u001b[0m         k: v\n\u001b[1;32m   2908\u001b[0m         \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariables\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   2909\u001b[0m         \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m variables \u001b[39mand\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude_vars\n\u001b[1;32m   2910\u001b[0m     }\n\u001b[0;32m-> 2911\u001b[0m     reindexed_vars \u001b[39m=\u001b[39m alignment\u001b[39m.\u001b[39;49mreindex_variables(\n\u001b[1;32m   2912\u001b[0m         to_reindex,\n\u001b[1;32m   2913\u001b[0m         dim_pos_indexers,\n\u001b[1;32m   2914\u001b[0m         copy\u001b[39m=\u001b[39;49maligner\u001b[39m.\u001b[39;49mcopy,\n\u001b[1;32m   2915\u001b[0m         fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[1;32m   2916\u001b[0m         sparse\u001b[39m=\u001b[39;49maligner\u001b[39m.\u001b[39;49msparse,\n\u001b[1;32m   2917\u001b[0m     )\n\u001b[1;32m   2918\u001b[0m     new_variables\u001b[39m.\u001b[39mupdate(reindexed_vars)\n\u001b[1;32m   2919\u001b[0m     new_coord_names \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coord_names \u001b[39m|\u001b[39m \u001b[39mset\u001b[39m(new_indexes)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/alignment.py:75\u001b[0m, in \u001b[0;36mreindex_variables\u001b[0;34m(variables, dim_pos_indexers, copy, fill_value, sparse)\u001b[0m\n\u001b[1;32m     72\u001b[0m needs_masking \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(d \u001b[39min\u001b[39;00m masked_dims \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m var\u001b[39m.\u001b[39mdims)\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m needs_masking:\n\u001b[0;32m---> 75\u001b[0m     new_var \u001b[39m=\u001b[39m var\u001b[39m.\u001b[39;49m_getitem_with_mask(indxr, fill_value\u001b[39m=\u001b[39;49mfill_value_)\n\u001b[1;32m     76\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mall\u001b[39m(is_full_slice(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m indxr):\n\u001b[1;32m     77\u001b[0m     \u001b[39m# no reindexing necessary\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[39m# here we need to manually deal with copying data, since\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[39m# we neither created a new ndarray nor used fancy indexing\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     new_var \u001b[39m=\u001b[39m var\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/variable.py:937\u001b[0m, in \u001b[0;36mVariable._getitem_with_mask\u001b[0;34m(self, key, fill_value)\u001b[0m\n\u001b[1;32m    933\u001b[0m     mask \u001b[39m=\u001b[39m indexing\u001b[39m.\u001b[39mcreate_mask(indexer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, data)\n\u001b[1;32m    934\u001b[0m     \u001b[39m# we need to invert the mask in order to pass data first. This helps\u001b[39;00m\n\u001b[1;32m    935\u001b[0m     \u001b[39m# pint to choose the correct unit\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     \u001b[39m# TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     data \u001b[39m=\u001b[39m duck_array_ops\u001b[39m.\u001b[39;49mwhere(np\u001b[39m.\u001b[39;49mlogical_not(mask), data, fill_value)\n\u001b[1;32m    938\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     \u001b[39m# array cannot be indexed along dimensions of size 0, so just\u001b[39;00m\n\u001b[1;32m    940\u001b[0m     \u001b[39m# build the mask directly instead.\u001b[39;00m\n\u001b[1;32m    941\u001b[0m     mask \u001b[39m=\u001b[39m indexing\u001b[39m.\u001b[39mcreate_mask(indexer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/duck_array_ops.py:302\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Three argument where() with better dtype promotion rules.\"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m xp \u001b[39m=\u001b[39m get_array_namespace(condition)\n\u001b[0;32m--> 302\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39mwhere(condition, \u001b[39m*\u001b[39mas_shared_dtype([x, y], xp\u001b[39m=\u001b[39;49mxp))\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/duck_array_ops.py:197\u001b[0m, in \u001b[0;36mas_shared_dtype\u001b[0;34m(scalars_or_arrays, xp)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mas_shared_dtype\u001b[39m(scalars_or_arrays, xp\u001b[39m=\u001b[39mnp):\n\u001b[1;32m    196\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Cast a arrays to a shared dtype using xarray's type promotion rules.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, array_type(\u001b[39m\"\u001b[39m\u001b[39mcupy\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m scalars_or_arrays):\n\u001b[1;32m    198\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mcupy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcp\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         arrays \u001b[39m=\u001b[39m [asarray(x, xp\u001b[39m=\u001b[39mcp) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m scalars_or_arrays]\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/duck_array_ops.py:197\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mas_shared_dtype\u001b[39m(scalars_or_arrays, xp\u001b[39m=\u001b[39mnp):\n\u001b[1;32m    196\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Cast a arrays to a shared dtype using xarray's type promotion rules.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, array_type(\u001b[39m\"\u001b[39;49m\u001b[39mcupy\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m scalars_or_arrays):\n\u001b[1;32m    198\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mcupy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcp\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         arrays \u001b[39m=\u001b[39m [asarray(x, xp\u001b[39m=\u001b[39mcp) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m scalars_or_arrays]\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/pycompat.py:66\u001b[0m, in \u001b[0;36marray_type\u001b[0;34m(mod)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39marray_type\u001b[39m(mod: ModType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DuckArrayTypes:\n\u001b[1;32m     65\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Quick wrapper to get the array class of the module.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m DuckArrayModule(mod)\u001b[39m.\u001b[39mtype\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/xarray/core/pycompat.py:37\u001b[0m, in \u001b[0;36mDuckArrayModule.__init__\u001b[0;34m(self, mod)\u001b[0m\n\u001b[1;32m     35\u001b[0m duck_array_type: DuckArrayTypes\n\u001b[1;32m     36\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     duck_array_module \u001b[39m=\u001b[39m import_module(mod)\n\u001b[1;32m     38\u001b[0m     duck_array_version \u001b[39m=\u001b[39m Version(duck_array_module\u001b[39m.\u001b[39m__version__)\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m mod \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdask\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1080\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1504\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1476\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1612\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_path = Path(\"data/sat1/stage_data.nc\")\n",
    "merge_dataset = xr.load_dataset(Path(\"data/sat1/data_unprocessed_500hz.nc\"))\n",
    "output_data = add_stage_dimension(data_path, merge_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"data/sat1/split_stage_data_unprocessed_500hz.nc\")\n",
    "output_data.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 100Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding stage changes\n",
      "Combining segments\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"data/sat1/stage_data.nc\")\n",
    "merge_dataset = xr.load_dataset(Path(\"data/sat1/data_unprocessed_100hz.nc\"))\n",
    "output_data = add_stage_dimension(data_path, merge_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"data/sat1/split_stage_data_unprocessed_100hz.nc\")\n",
    "output_data.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 13:35:11.961822: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 13:35:12.676477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-16 13:35:14.636565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 13:35:14.661612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 13:35:14.661684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 13:35:14.663767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 13:35:14.663830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 13:35:14.663872: I tensorflow/compile"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n",
      "env: TF_GPU_ALLOCATOR=cuda_malloc_async\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 13:35:15.192557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 13:35:15.192639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 13:35:15.192647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-16 13:35:15.192710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 13:35:15.192739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21598 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from shared.data import add_stage_dimension\n",
    "from shared.training import split_data_on_participants, train_and_evaluate, k_fold_cross_validate, compile_kwargs\n",
    "from shared.normalization import *\n",
    "from shared.models import SAT1Base, SAT1Topological, SAT1Deep\n",
    "from shared.utilities import print_results\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "%env TF_GPU_ALLOCATOR=cuda_malloc_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = Path(\"logs/exp_preprocessing/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2a: Processed 100Hz (control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/sat1/split_stage_data.nc\")\n",
    "data = xr.load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: test fold: ['0009' '0017' '0001' '0024' '0012']\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 13:35:20.327635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-10-16 13:35:20.958524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-10-16 13:35:20.960666: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f9880014400 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-16 13:35:20.960690: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-10-16 13:35:20.964602: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-16 13:35:21.062440: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/git/hmp-ai/exp_preprocessing.ipynb Cell 21\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcompile_kwargs)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m train_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlogs_path\u001b[39m\u001b[39m\"\u001b[39m: logs_path,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39madditional_info\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mpreprocessing\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mdefault_100hz\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39madditional_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpreprocessing-default_100hz\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m }\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m results \u001b[39m=\u001b[39m k_fold_cross_validate(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     data, model, \u001b[39m5\u001b[39;49m, normalization_fn\u001b[39m=\u001b[39;49mnorm_min1_to_1, train_kwargs\u001b[39m=\u001b[39;49mtrain_kwargs\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m print_results(results)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdel\u001b[39;00m model\n",
      "File \u001b[0;32m/mnt/c/git/hmp-ai/shared/training.py:248\u001b[0m, in \u001b[0;36mk_fold_cross_validate\u001b[0;34m(data, model, k, batch_size, epochs, workers, normalization_fn, gen_kwargs, train_kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m test_data \u001b[39m=\u001b[39m normalization_fn(test_data, train_min, train_max)\n\u001b[1;32m    247\u001b[0m \u001b[39m# Train model and test\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m result \u001b[39m=\u001b[39m train_and_evaluate(\n\u001b[1;32m    249\u001b[0m     model,\n\u001b[1;32m    250\u001b[0m     train_data,\n\u001b[1;32m    251\u001b[0m     test_data,\n\u001b[1;32m    252\u001b[0m     val\u001b[39m=\u001b[39;49mtest_data,\n\u001b[1;32m    253\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    254\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    255\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    256\u001b[0m     gen_kwargs\u001b[39m=\u001b[39;49mgen_kwargs,\n\u001b[1;32m    257\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_kwargs,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    260\u001b[0m \u001b[39m# Add test results to list\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFold \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: accuracy: \u001b[39m\u001b[39m{\u001b[39;00mresult[\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/git/hmp-ai/shared/training.py:140\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train, test, val, batch_size, epochs, workers, logs_path, additional_info, additional_name, generator, gen_kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(LoggingTensorBoard(to_write, log_dir\u001b[39m=\u001b[39mpath))\n\u001b[1;32m    139\u001b[0m use_multiprocessing \u001b[39m=\u001b[39m workers \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 140\u001b[0m fit \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    141\u001b[0m     train_gen,\n\u001b[1;32m    142\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    143\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    144\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_gen,\n\u001b[1;32m    145\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m    146\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    147\u001b[0m )\n\u001b[1;32m    149\u001b[0m \u001b[39m# Test model and write test summary\u001b[39;00m\n\u001b[1;32m    150\u001b[0m test_args \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:890\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[39m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 890\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    891\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    893\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_function_spec  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    894\u001b[0m       \u001b[39m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    895\u001b[0m           args, kwds))\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = SAT1Base(len(data.channels), len(data.samples), len(data.labels))\n",
    "model.compile(**compile_kwargs)\n",
    "train_kwargs = {\n",
    "    \"logs_path\": logs_path,\n",
    "    \"additional_info\": {\"preprocessing\": \"default_100hz\"},\n",
    "    \"additional_name\": f\"preprocessing-default_100hz\",\n",
    "}\n",
    "results = k_fold_cross_validate(\n",
    "    data, model, 5, normalization_fn=norm_min1_to_1, train_kwargs=train_kwargs\n",
    ")\n",
    "print_results(results)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2b: Unprocessed 100Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/sat1/split_stage_data_unprocessed_100hz.nc\")\n",
    "data = xr.load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: test fold: ['0009' '0017' '0001' '0024' '0012']\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 13:27:15.193660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-10-16 13:27:16.107853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-10-16 13:27:16.110156: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5587931d77a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-16 13:27:16.110180: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-10-16 13:27:16.114030: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-16 13:27:16.210419: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999/999 [==============================] - 17s 13ms/step - loss: 0.9601 - accuracy: 0.6227 - val_loss: 0.7204 - val_accuracy: 0.7413\n",
      "Epoch 2/10\n",
      "999/999 [==============================] - 13s 13ms/step - loss: 0.6628 - accuracy: 0.7553 - val_loss: 0.5544 - val_accuracy: 0.7966\n",
      "Epoch 3/10\n",
      "999/999 [==============================] - 13s 13ms/step - loss: 0.5639 - accuracy: 0.7889 - val_loss: 0.5042 - val_accuracy: 0.8150\n",
      "Epoch 4/10\n",
      "999/999 [==============================] - 13s 13ms/step - loss: 0.5248 - accuracy: 0.8052 - val_loss: 0.4940 - val_accuracy: 0.8147\n",
      "Epoch 5/10\n",
      "999/999 [==============================] - 13s 13ms/step - loss: 0.4889 - accuracy: 0.8203 - val_loss: 0.4835 - val_accuracy: 0.8190\n",
      "Epoch 6/10\n",
      "999/999 [==============================] - 13s 13ms/step - loss: 0.4620 - accuracy: 0.8303 - val_loss: 0.4493 - val_accuracy: 0.8347\n",
      "Epoch 7/10\n",
      "999/999 [==============================] - 13s 13ms/step - loss: 0.4315 - accuracy: 0.8400 - val_loss: 0.4536 - val_accuracy: 0.8349\n",
      "Epoch 8/10\n",
      "999/999 [==============================] - 14s 13ms/step - loss: 0.4179 - accuracy: 0.8451 - val_loss: 0.4284 - val_accuracy: 0.8454\n",
      "Epoch 9/10\n",
      "999/999 [==============================] - 13s 13ms/step - loss: 0.3967 - accuracy: 0.8554 - val_loss: 0.4675 - val_accuracy: 0.8324\n",
      "Epoch 10/10\n",
      "999/999 [==============================] - 13s 13ms/step - loss: 0.3809 - accuracy: 0.8607 - val_loss: 0.4362 - val_accuracy: 0.8401\n",
      "251/251 [==============================] - 3s 11ms/step\n",
      "Fold 1: accuracy: 0.8401394422310757\n",
      "Fold 2: test fold: ['0010' '0014' '0002' '0023' '0006']\n",
      "Epoch 1/10\n",
      "994/994 [==============================] - 14s 13ms/step - loss: 1.0001 - accuracy: 0.6010 - val_loss: 0.7279 - val_accuracy: 0.7283\n",
      "Epoch 2/10\n",
      "994/994 [==============================] - 13s 13ms/step - loss: 0.7013 - accuracy: 0.7393 - val_loss: 0.6133 - val_accuracy: 0.7666\n",
      "Epoch 3/10\n",
      "994/994 [==============================] - 13s 13ms/step - loss: 0.6268 - accuracy: 0.7664 - val_loss: 0.5625 - val_accuracy: 0.7952\n",
      "Epoch 4/10\n",
      "994/994 [==============================] - 14s 14ms/step - loss: 0.5720 - accuracy: 0.7906 - val_loss: 0.5054 - val_accuracy: 0.8157\n",
      "Epoch 5/10\n",
      "994/994 [==============================] - 14s 14ms/step - loss: 0.5236 - accuracy: 0.8072 - val_loss: 0.4761 - val_accuracy: 0.8259\n",
      "Epoch 6/10\n",
      "994/994 [==============================] - 14s 13ms/step - loss: 0.4963 - accuracy: 0.8162 - val_loss: 0.4568 - val_accuracy: 0.8308\n",
      "Epoch 7/10\n",
      "994/994 [==============================] - 14s 13ms/step - loss: 0.4727 - accuracy: 0.8289 - val_loss: 0.4445 - val_accuracy: 0.8450\n",
      "Epoch 8/10\n",
      "994/994 [==============================] - 14s 13ms/step - loss: 0.4538 - accuracy: 0.8355 - val_loss: 0.4366 - val_accuracy: 0.8428\n",
      "Epoch 9/10\n",
      "994/994 [==============================] - 14s 13ms/step - loss: 0.4308 - accuracy: 0.8458 - val_loss: 0.4307 - val_accuracy: 0.8503\n",
      "Epoch 10/10\n",
      "994/994 [==============================] - 14s 13ms/step - loss: 0.4181 - accuracy: 0.8459 - val_loss: 0.4073 - val_accuracy: 0.8511\n",
      "256/256 [==============================] - 3s 10ms/step\n",
      "Fold 2: accuracy: 0.85107421875\n",
      "Fold 3: test fold: ['0003' '0013' '0016' '0004' '0005']\n",
      "Epoch 1/10\n",
      "   5/1000 [..............................] - ETA: 15s - loss: 1.7046 - accuracy: 0.3750 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_train_batch_end` time: 0.0082s). Check your callbacks.\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 1.0161 - accuracy: 0.5976 - val_loss: 0.7796 - val_accuracy: 0.7028\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.7075 - accuracy: 0.7388 - val_loss: 0.6059 - val_accuracy: 0.7626\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 14s 13ms/step - loss: 0.6316 - accuracy: 0.7639 - val_loss: 0.6032 - val_accuracy: 0.7728\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 14s 13ms/step - loss: 0.5826 - accuracy: 0.7849 - val_loss: 0.5454 - val_accuracy: 0.7949\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.5351 - accuracy: 0.8056 - val_loss: 0.4930 - val_accuracy: 0.8155\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 14s 13ms/step - loss: 0.5067 - accuracy: 0.8172 - val_loss: 0.6478 - val_accuracy: 0.7598\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 14s 13ms/step - loss: 0.4800 - accuracy: 0.8249 - val_loss: 0.4667 - val_accuracy: 0.8288\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 14s 13ms/step - loss: 0.4543 - accuracy: 0.8363 - val_loss: 0.4807 - val_accuracy: 0.8213\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 14s 13ms/step - loss: 0.4266 - accuracy: 0.8458 - val_loss: 0.4582 - val_accuracy: 0.8316\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4175 - accuracy: 0.8489 - val_loss: 0.4471 - val_accuracy: 0.8389\n",
      "249/249 [==============================] - 3s 11ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24161/1555984275.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"logs_path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlogs_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"additional_info\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"preprocessing\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"unprocessed_100hz\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"additional_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"preprocessing-unprocessed_100hz\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[0;32m----> 9\u001b[0;31m results = k_fold_cross_validate(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_min1_to_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/git/hmp-ai/shared/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data, model, k, batch_size, epochs, workers, normalization_fn, gen_kwargs, train_kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalization_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalization_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# Train model and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         result = train_and_evaluate(\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/git/hmp-ai/shared/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, train, test, val, batch_size, epochs, workers, logs_path, additional_info, additional_name, generator, gen_kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# Test model and write test summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mtest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrite_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mtest_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logs_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_report\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrite_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/git/hmp-ai/shared/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, test_gen, logs_path, log_report)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;34m\"If log_report is set to True, the parameter logs_path must be provided\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             )\n\u001b[1;32m    184\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mpredicted_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0mpredicted_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     test_results = classification_report(\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2584\u001b[0m                     \u001b[0;34m\"information of where went wrong, or file a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m                     \u001b[0;34m\"issue/bug to `tf.keras`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2586\u001b[0m                 )\n\u001b[1;32m   2587\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2588\u001b[0;31m         all_outputs = tf.__internal__.nest.map_structure_up_to(\n\u001b[0m\u001b[1;32m   2589\u001b[0m             \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpotentially_ragged_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2590\u001b[0m         )\n\u001b[1;32m   2591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m   \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0mof\u001b[0m \u001b[0mrepeatedly\u001b[0m \u001b[0mapplying\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mstructure\u001b[0m \u001b[0mlayout\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mshallow_tree\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m   \"\"\"\n\u001b[0;32m-> 1030\u001b[0;31m   return nest_util.map_structure_up_to(\n\u001b[0m\u001b[1;32m   1031\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m       \u001b[0mshallow_tree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m       \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Discards the path arg.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(modality, shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1634\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0mof\u001b[0m \u001b[0mrepeatedly\u001b[0m \u001b[0mapplying\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mstructure\u001b[0m \u001b[0mlayout\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mshallow_tree\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m   \"\"\"\n\u001b[1;32m   1637\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1638\u001b[0;31m     return _tf_core_map_structure_with_tuple_paths_up_to(\n\u001b[0m\u001b[1;32m   1639\u001b[0m         \u001b[0mshallow_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m     )\n\u001b[1;32m   1641\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1683\u001b[0m       for path, _ in _tf_core_yield_flat_up_to(\n\u001b[1;32m   1684\u001b[0m           \u001b[0mshallow_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_nested_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m       )\n\u001b[1;32m   1686\u001b[0m   )\n\u001b[0;32m-> 1687\u001b[0;31m   results = [\n\u001b[0m\u001b[1;32m   1688\u001b[0m       \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_path_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_value_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m   ]\n\u001b[1;32m   1690\u001b[0m   return _tf_core_pack_sequence_as(\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1687\u001b[0m def _tf_core_map_structure_with_tuple_paths_up_to(\n\u001b[0;32m-> 1688\u001b[0;31m     \u001b[0mshallow_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m ):\n\u001b[1;32m   1690\u001b[0m   \u001b[0;34m\"\"\"See comments for map_structure_with_tuple_paths_up_to() in tensorflow/python/util/nest.py.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(_, *values)\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m       \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Discards the path arg.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   4136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4140\u001b[0;31m     \u001b[0mnon_batch_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4141\u001b[0m     constant_dims = tf.math.reduce_all(\n\u001b[1;32m   4142\u001b[0m         \u001b[0mnon_batch_shapes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnon_batch_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4143\u001b[0m     )\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/ops/array_ops_stack.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0;31m# If the input is a constant list, it can be converted to a constant op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0mvalue_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1439\u001b[0m ):\n\u001b[1;32m   1440\u001b[0m   \u001b[0;34m\"\"\"Implementation of the public convert_to_tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m   1444\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   )\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    230\u001b[0m                   \u001b[0;34mf\"actual = {ret.dtype.base_dtype.name}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                   name=name))\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1543\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1547\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1450\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m     \u001b[0;31m# NOTE: Fast path when all the items are tensors, this doesn't do any type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m     \u001b[0;31m# checking.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_or_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_or_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m   \u001b[0mmust_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m   \u001b[0mconverted_elems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6575\u001b[0m         _ctx, \"Pack\", name, values, \"axis\", axis)\n\u001b[1;32m   6576\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6577\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6578\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6579\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6580\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6581\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6582\u001b[0m       return pack_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = SAT1Base(len(data.channels), len(data.samples), len(data.labels))\n",
    "model.compile(**compile_kwargs)\n",
    "train_kwargs = {\n",
    "    \"logs_path\": logs_path,\n",
    "    \"additional_info\": {\"preprocessing\": \"unprocessed_100hz\"},\n",
    "    \"additional_name\": f\"preprocessing-unprocessed_100hz\",\n",
    "}\n",
    "results = k_fold_cross_validate(\n",
    "    data, model, 5, normalization_fn=norm_min1_to_1, train_kwargs=train_kwargs\n",
    ")\n",
    "print_results(results)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2c: Unprocessed 500Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/sat1/split_stage_data_unprocessed_500hz.nc\")\n",
    "data = xr.load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: test fold: ['0009' '0017' '0001' '0024' '0012']\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-13 14:58:39.461825: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999/999 [==============================] - 49s 45ms/step - loss: 1.2078 - accuracy: 0.4944\n",
      "Epoch 2/10\n",
      "999/999 [==============================] - 45s 45ms/step - loss: 1.0380 - accuracy: 0.5850\n",
      "Epoch 3/10\n",
      "999/999 [==============================] - 44s 44ms/step - loss: 0.8440 - accuracy: 0.6839\n",
      "Epoch 4/10\n",
      "999/999 [==============================] - 44s 43ms/step - loss: 0.7151 - accuracy: 0.7388\n",
      "Epoch 5/10\n",
      "999/999 [==============================] - 44s 43ms/step - loss: 0.6575 - accuracy: 0.7577\n",
      "Epoch 6/10\n",
      "999/999 [==============================] - 44s 44ms/step - loss: 0.6195 - accuracy: 0.7760\n",
      "Epoch 7/10\n",
      "999/999 [==============================] - 43s 43ms/step - loss: 0.5738 - accuracy: 0.7956\n",
      "Epoch 8/10\n",
      "999/999 [==============================] - 44s 43ms/step - loss: 0.5430 - accuracy: 0.8089\n",
      "Epoch 9/10\n",
      "999/999 [==============================] - 44s 43ms/step - loss: 0.5072 - accuracy: 0.8171\n",
      "Epoch 10/10\n",
      "999/999 [==============================] - 44s 44ms/step - loss: 0.5183 - accuracy: 0.8172\n",
      "251/251 [==============================] - 3s 11ms/step\n",
      "Fold 1: accuracy: 0.8122509960159362\n",
      "Fold 2: test fold: ['0010' '0014' '0002' '0023' '0006']\n",
      "Epoch 1/10\n",
      "994/994 [==============================] - 44s 44ms/step - loss: 1.5139 - accuracy: 0.3607\n",
      "Epoch 2/10\n",
      "994/994 [==============================] - 46s 46ms/step - loss: 1.1782 - accuracy: 0.5015\n",
      "Epoch 3/10\n",
      "994/994 [==============================] - 45s 45ms/step - loss: 1.0739 - accuracy: 0.5689\n",
      "Epoch 4/10\n",
      "994/994 [==============================] - 46s 46ms/step - loss: 0.8898 - accuracy: 0.6524\n",
      "Epoch 5/10\n",
      "994/994 [==============================] - 46s 45ms/step - loss: 0.7438 - accuracy: 0.7207\n",
      "Epoch 6/10\n",
      "994/994 [==============================] - 46s 46ms/step - loss: 0.6960 - accuracy: 0.7433\n",
      "Epoch 7/10\n",
      "994/994 [==============================] - 46s 46ms/step - loss: 0.6611 - accuracy: 0.7603\n",
      "Epoch 8/10\n",
      "994/994 [==============================] - 45s 45ms/step - loss: 0.6310 - accuracy: 0.7654\n",
      "Epoch 9/10\n",
      "994/994 [==============================] - 45s 45ms/step - loss: 0.6187 - accuracy: 0.7728\n",
      "Epoch 10/10\n",
      "994/994 [==============================] - 45s 45ms/step - loss: 0.6002 - accuracy: 0.7789\n",
      "256/256 [==============================] - 3s 12ms/step\n",
      "Fold 2: accuracy: 0.808349609375\n",
      "Fold 3: test fold: ['0003' '0013' '0016' '0004' '0005']\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 46s 45ms/step - loss: 1.6022 - accuracy: 0.2300\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 1.2539 - accuracy: 0.4436\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 46s 45ms/step - loss: 1.0896 - accuracy: 0.5592\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 46s 45ms/step - loss: 0.9461 - accuracy: 0.6291\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 46s 45ms/step - loss: 0.7542 - accuracy: 0.7201\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.6923 - accuracy: 0.7464\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 46s 45ms/step - loss: 0.6593 - accuracy: 0.7581\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.6261 - accuracy: 0.7722\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.6074 - accuracy: 0.7801\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 46s 45ms/step - loss: 0.5935 - accuracy: 0.7851\n",
      "249/249 [==============================] - 3s 12ms/step\n",
      "Fold 3: accuracy: 0.7899096385542169\n",
      "Fold 4: test fold: ['0021' '0018' '0022' '0019' '0025']\n",
      "Epoch 1/10\n",
      "   6/1008 [..............................] - ETA: 48s - loss: 4.3446 - accuracy: 0.1771"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/git/hmp-ai/exp_preprocessing.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mNadam(),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m train_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlogs_path\u001b[39m\u001b[39m\"\u001b[39m: logs_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39madditional_info\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mpreprocessing\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39munprocessed_500hz\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39madditional_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpreprocessing-unprocessed_500hz\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m }\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m results \u001b[39m=\u001b[39m k_fold_cross_validate(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     data, model, \u001b[39m5\u001b[39;49m, normalization_fn\u001b[39m=\u001b[39;49mnorm_min1_to_1, train_kwargs\u001b[39m=\u001b[39;49mtrain_kwargs\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m print_results(results)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/git/hmp-ai/exp_preprocessing.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdel\u001b[39;00m model\n",
      "File \u001b[0;32m/mnt/c/git/hmp-ai/shared/training.py:242\u001b[0m, in \u001b[0;36mk_fold_cross_validate\u001b[0;34m(data, model, k, batch_size, epochs, workers, normalization_fn, gen_kwargs, train_kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m test_data \u001b[39m=\u001b[39m normalization_fn(test_data, train_min, train_max)\n\u001b[1;32m    241\u001b[0m \u001b[39m# Train model and test\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m result \u001b[39m=\u001b[39m train_and_evaluate(\n\u001b[1;32m    243\u001b[0m     model,\n\u001b[1;32m    244\u001b[0m     train_data,\n\u001b[1;32m    245\u001b[0m     test_data,\n\u001b[1;32m    246\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    247\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    248\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    249\u001b[0m     gen_kwargs\u001b[39m=\u001b[39;49mgen_kwargs,\n\u001b[1;32m    250\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_kwargs\n\u001b[1;32m    251\u001b[0m )\n\u001b[1;32m    253\u001b[0m \u001b[39m# Add test results to list\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFold \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: accuracy: \u001b[39m\u001b[39m{\u001b[39;00mresult[\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/git/hmp-ai/shared/training.py:134\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train, test, val, batch_size, epochs, workers, logs_path, additional_info, additional_name, generator, gen_kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(LoggingTensorBoard(to_write, log_dir\u001b[39m=\u001b[39mpath))\n\u001b[1;32m    133\u001b[0m use_multiprocessing \u001b[39m=\u001b[39m workers \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 134\u001b[0m fit \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    135\u001b[0m     train_gen,\n\u001b[1;32m    136\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    137\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    138\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_gen,\n\u001b[1;32m    139\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m    140\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[39m# Test model and write test summary\u001b[39;00m\n\u001b[1;32m    144\u001b[0m test_args \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = SAT1Deep(len(data.channels), len(data.samples), len(data.labels))\n",
    "model.compile(**compile_kwargs)\n",
    "train_kwargs = {\n",
    "    \"logs_path\": logs_path,\n",
    "    \"additional_info\": {\"preprocessing\": \"unprocessed_500hz\"},\n",
    "    \"additional_name\": f\"preprocessing-unprocessed_500hz\",\n",
    "}\n",
    "results = k_fold_cross_validate(\n",
    "    data, model, 5, normalization_fn=norm_min1_to_1, train_kwargs=train_kwargs\n",
    ")\n",
    "print_results(results)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-25 13:59:04.226414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.13.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# View results in Tensorboard\n",
    "! tensorboard --logdir logs/exp_preprocessing/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
